from __future__ import print_functionimport argparsefrom math import log10, ceilimport random, shutil, jsonfrom os.path import join, exists, isfile, realpath, dirnameimport osimport torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimfrom torch.autograd import Variablefrom torch.utils.data import DataLoaderfrom torch.utils.data.sampler import SubsetRandomSamplerfrom torch.utils.data.dataset import Subsetimport torchvision.transforms as transformsfrom PIL import Imagefrom datetime import datetimeimport torchvision.datasets as datasetsimport torchvision.models as modelsimport h5pyimport faissfrom tensorboardX import SummaryWriterimport numpy as npfrom Place365 import wideresnetfrom netVLAD import netvladfrom netVLADbase import netVLADbaseResNetfrom torchvision import transforms as trnfrom torch.autograd import Variable as Vfrom scipy.misc import imresize as imresizeimport cv2import warningsparser = argparse.ArgumentParser(description='pytorch-NetVlad')parser.add_argument('--cacheBatchSize', type=int, default=4, help='Batch size for caching and testing')#24parser.add_argument('--nGPU', type=int, default=1, help='number of GPU to use.')parser.add_argument('--nocuda', action='store_true', help='Dont use cuda')parser.add_argument('--threads', type=int, default=8, help='Number of threads for each data loader to use')parser.add_argument('--seed', type=int, default=123, help='Random seed to use.')parser.add_argument('--dataPath', type=str, default='data/', help='Path for centroid data.')parser.add_argument('--runsPath', type=str, default='runs/', help='Path to save runs to.')parser.add_argument('--cachePath', type=str, default='/tmp/', help='Path to save cache to.')parser.add_argument('--resume', type=str, default='',                    help='Path to load checkpoint from, for resuming training or testing.')parser.add_argument('--ckpt', type=str, default='latest',                    help='Resume from latest or best checkpoint.', choices=['latest', 'best'])parser.add_argument('--dataset', type=str, default='Yuquan',                    help='Dataset to use', choices=['pittsburgh', 'MOLP', 'Yuquan'])parser.add_argument('--pooling', type=str, default='netvlad', help='type of pooling to use',                    choices=['netvlad', 'max', 'avg'])parser.add_argument('--num_clusters', type=int, default=64, help='Number of NetVlad clusters. Default=64')parser.add_argument('--split', type=str, default='val', help='Data split to use for testing. Default is val',                    choices=['test', 'test250k', 'train', 'val'])netVLADtrainNum = 2def load_labels():    # prepare all the labels    # scene category relevant    file_name_category = 'Place365/categories_places365.txt'    classes = list()    with open(file_name_category) as class_file:        for line in class_file:            classes.append(line.strip().split(' ')[0][3:])    classes = tuple(classes)    # indoor and outdoor relevant    file_name_IO = 'Place365/IO_places365.txt'    with open(file_name_IO) as f:        lines = f.readlines()        labels_IO = []        for line in lines:            items = line.rstrip().split()            labels_IO.append(int(items[-1]) -1) # 0 is indoor, 1 is outdoor    labels_IO = np.array(labels_IO)    # scene attribute relevant    file_name_attribute = 'Place365/labels_sunattribute.txt'    with open(file_name_attribute) as f:        lines = f.readlines()        labels_attribute = [item.rstrip() for item in lines]    file_name_W = 'Place365/W_sceneattribute_wideresnet18.npy'    W_attribute = np.load(file_name_W)    return classes, labels_IO, labels_attribute, W_attributedef returnTF():#load the image transformer    tf = trn.Compose([        trn.Resize((224,224)),        trn.ToTensor(),        trn.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])    ])    return tfdef returnCAM(feature_conv, weight_softmax, class_idx):    # generate the class activation maps upsample to 256x256    size_upsample = (256, 256)    nc, h, w = feature_conv.shape    output_cam = []    for idx in class_idx:        cam = weight_softmax[class_idx].dot(feature_conv.reshape((nc, h*w)))        cam = cam.reshape(h, w)        cam = cam - np.min(cam)        cam_img = cam / np.max(cam)        cam_img = np.uint8(255 * cam_img)        output_cam.append(imresize(cam_img, size_upsample))    return output_camdef testDataset(eval_set, epoch=0, write_tboard=False):    # TODO what if features dont fit in memory?    test_data_loader = DataLoader(dataset=eval_set,                                  num_workers=opt.threads, batch_size=opt.cacheBatchSize, shuffle=False,                                  pin_memory=cuda)    model.eval()    with torch.no_grad():#不会反向传播，提高inference速度        print('====> Extracting Features')        pool_size = encoder_dim        if opt.pooling.lower() == 'netvlad': pool_size *= opt.num_clusters        dbFeat = np.empty((len(eval_set), pool_size))        for iteration, (input, indices) in enumerate(test_data_loader, 1):            input = input.to(device)            input1 = input#[ :, :, :, 0:224]            # input2 = input[ :, :, :, 224:448]            # input3 = input[ :, :, :, 448:672]            # input4 = input[ :, :, :, 672:896]            # input5 = input[:, :, :, 852:1065]            # forward pass            logit = modelPlaces.forward(input1)            # logit = modelPlaces.forward(input2)            # logit = modelPlaces.forward(input3)            # logit = modelPlaces.forward(input4)            # logit = modelPlaces.forward(input5)            image_encoding1 = model.encoder(netVLADlayer_input[0])            vlad_encoding1 = model.pool(image_encoding1)            # image_encoding2 = model.encoder(netVLADlayer_input[1])            # vlad_encoding2 = model.pool(image_encoding2)            # image_encoding3 = model.encoder(netVLADlayer_input[2])            # vlad_encoding3 = model.pool(image_encoding3)            # image_encoding4 = model.encoder(netVLADlayer_input[3])            # vlad_encoding4 = model.pool(image_encoding4)            # image_encoding5 = model.encoder(netVLADlayer_input[4])            # vlad_encoding5 = model.pool(image_encoding5)            netVLADlayer_input.clear()            # if  iteration<eval_set.numDb:            #     vlad_encoding=torch.cat((vlad_encoding1,vlad_encoding2),1)            # else:            #     vlad_encoding = torch.cat((vlad_encoding2, vlad_encoding1),1)            vlad_encoding = vlad_encoding1#+vlad_encoding2+vlad_encoding3+vlad_encoding4#+vlad_encoding5            dbFeat[indices.detach().numpy(), :] = vlad_encoding.detach().cpu().numpy()            if iteration % 50 == 0 or len(test_data_loader) <= 10:                print("==> Batch ({}/{})".format(iteration,                                                 len(test_data_loader)), flush=True)            del input, vlad_encoding, image_encoding1,vlad_encoding1#,image_encoding2,vlad_encoding2,image_encoding3,vlad_encoding3,vlad_encoding4,image_encoding4            #,image_encoding5,vlad_encoding5    del test_data_loader    # extracted for both db and query, now split in own sets    #qFeat = dbFeat[eval_set.dbStruct.numDb:].astype('float32')#float32    #dbFeat = dbFeat[:eval_set.dbStruct.numDb].astype('float32')#float32    qFeat = dbFeat[eval_set.numDb:].astype('float32')#float32    dbFeat = dbFeat[:eval_set.numDb].astype('float32')#float32    np.savetxt("query-afternoon2.txt", qFeat)    np.savetxt("database-afternoon1.txt", dbFeat)    print('====> Building faiss index')    faiss_index = faiss.IndexFlatL2(pool_size)    faiss_index.add(dbFeat)    print('====> Calculating recall @ N')    #n_values = [1, 5, 10, 20]    distances, predictions = faiss_index.search(qFeat, 1)    fp = 0    tp = 0    for i in range(0,len(predictions)):        if abs(predictions[i] - i)<=5:            tp=tp+1        else:            fp=fp+1    precision = tp/(tp+fp)    recall = 1    f1=2*precision/(precision+recall)    print(['F1=', f1 ])    return distances, predictions    #_, predictions = faiss_index.search(qFeat, max(n_values))    # # for each query get those within threshold distance    # gt = eval_set.getPositives()    #    # correct_at_n = np.zeros(len(n_values))    # # TODO can we do this on the matrix in one go?    # for qIx, pred in enumerate(predictions):    #     for i, n in enumerate(n_values):    #         # if in top N then also in top NN, where NN > N    #         if np.any(np.in1d(pred[:n], gt[qIx])):    #             correct_at_n[i:] += 1    #             break    # recall_at_n = correct_at_n / eval_set.dbStruct.numQ    #    # recalls = {}  # make dict for output    # for i, n in enumerate(n_values):    #     recalls[n] = recall_at_n[i]    #     print("====> Recall@{}: {:.4f}".format(n, recall_at_n[i]))    #return recallsdef test():    # load the labels    classes, labels_IO, labels_attribute, W_attribute = load_labels()    # load the model 已经加载完毕    # load the transformer    tf = returnTF()  # image transformer    # get the softmax weight    params = list(modelPlaces.parameters())    weight_softmax = params[-2].data.numpy()    weight_softmax[weight_softmax < 0] = 0    # load the test image    img_url = 'http://places.csail.mit.edu/demo/5.jpg'    os.system('wget %s -q -O test.jpg' % img_url)    img = Image.open('test.jpg')    input_img = V(tf(img).unsqueeze(0))    # forward pass    logit = modelPlaces.forward(input_img)    h_x = F.softmax(logit, 1).data.squeeze()    probs, idx = h_x.sort(0, True)    probs = probs.numpy()    idx = idx.numpy()    print('RESULT ON ' + img_url)    # output the IO prediction    io_image = np.mean(labels_IO[idx[:10]])  # vote for the indoor or outdoor    if io_image < 0.5:        print('--TYPE OF ENVIRONMENT: indoor')    else:        print('--TYPE OF ENVIRONMENT: outdoor')    # output the prediction of scene category    print('--SCENE CATEGORIES:')    for i in range(0, 5):        print('{:.3f} -> {}'.format(probs[i], classes[idx[i]]))    # output the scene attributes    responses_attribute = W_attribute.dot(features_blobs[1])    idx_a = np.argsort(responses_attribute)    print('--SCENE ATTRIBUTES:')    print(', '.join([labels_attribute[idx_a[i]] for i in range(-1, -10, -1)]))    # generate class activation mapping    print('Class activation map is saved as cam.jpg')    CAMs = returnCAM(features_blobs[0], weight_softmax, [idx[0]])    # render the CAM and output    img = cv2.imread('test.jpg')    height, width, _ = img.shape    heatmap = cv2.applyColorMap(cv2.resize(CAMs[0], (width, height)), cv2.COLORMAP_JET)    result = heatmap * 0.4 + img * 0.5    cv2.imwrite('cam.jpg', result)class Flatten(nn.Module):    def forward(self, input):        return input.view(input.size(0), -1)class L2Norm(nn.Module):    def __init__(self, dim=1):        super().__init__()        self.dim = dim    def forward(self, input):        return F.normalize(input, p=2, dim=self.dim)def hook_feature(module, input, output):    features_blobs.append(np.squeeze(output.data.cpu().numpy()))def hook_layer(module, input, output):    netVLADlayer_input.append(output)#构造场景识别/场景属性的resnet模型，并加载参数；同时增加netvlad分支所用的hookdef loadSceneModel(trainedNetVLADLayers):    # this model has a last conv feature map as 14x14    model_file = 'Place365/wideresnet18_places365.pth.tar'    model = wideresnet.resnet18(num_classes=365)    # load object saved with torch.save() from a file, with funtion specifiying how to remap storage locations in the parameter list    checkpoint = torch.load(model_file, map_location=lambda storage, loc: storage) #gpu->cpu, why?!    state_dict = {str.replace(k,'module.',''): v for k,v in checkpoint['state_dict'].items()} # 去掉字符串 ‘module.’    model.load_state_dict(state_dict)    model.eval()    # hook the feature extractor    features_names = ['layer4','avgpool'] # this is the last conv layer of the resnet    for name in features_names:        model._modules.get(name).register_forward_hook(hook_feature)    # hook the layer for netVLAD    features_candidates = ['layer4', 'layer3', 'layer2', 'layer1', 'relu1']    model._modules.get(features_candidates[trainedNetVLADLayers]).register_forward_hook(hook_layer)    return modelif __name__ == "__main__":    # 屏蔽warnings -- UserWarning: Loky-backed parallel loops cannot be called in a multiprocessing, setting n_jobs=1    warnings.filterwarnings("ignore")    opt = parser.parse_args()    restore_var = ['runsPath', 'savePath', 'arch', 'num_clusters', 'pooling', 'seed']## 指定数据集    if opt.dataset.lower() == 'pittsburgh':        from netVLAD import pittsburgh as dataset    elif opt.dataset.lower() == 'molp':        from netVLAD import MOLP as dataset    elif opt.dataset.lower() == 'yuquan':        from netVLAD import Yuquan as dataset    else:        raise Exception('Unknown dataset')## 指定设备    cuda = not opt.nocuda    if cuda and not torch.cuda.is_available():        raise Exception("No GPU found, please run with --nocuda")    device = torch.device("cuda" if cuda else "cpu")    random.seed(opt.seed)    np.random.seed(opt.seed)    torch.manual_seed(opt.seed)    if cuda:        torch.cuda.manual_seed(opt.seed)## 导入数据集    print('===> Loading dataset(s)')    if opt.split.lower() == 'test':        whole_test_set = dataset.get_whole_test_set()        print('===> Evaluating on test set')    elif opt.split.lower() == 'test250k':        whole_test_set = dataset.get_250k_test_set()        print('===> Evaluating on test250k set')    elif opt.split.lower() == 'train':        whole_test_set = dataset.get_whole_training_set()        print('===> Evaluating on train set')    elif opt.split.lower() == 'val':        whole_test_set = dataset.get_whole_val_set()        print('===> Evaluating on val set')    else:        raise ValueError('Unknown dataset split: ' + opt.split)    #print('====> Query count:', whole_test_set.dbStruct.numQ)    print('====> Query count:', whole_test_set.numQ)## 构造网络模型    print('===> Building model')    # 构造基于resnet18的场景分类/场景属性    features_blobs = []    netVLADlayer_input = [] #netVLAD分支的输入值    modelPlaces = loadSceneModel(netVLADtrainNum)    modelPlaces = modelPlaces.to(device)    model = netVLADbaseResNet(wideresnet.BasicBlock, [2, 2, 2, 2], netVLADtrain=netVLADtrainNum)    encoder_dim = 512    model_file = 'Place365/wideresnet18_places365.pth.tar'    # load object saved with torch.save() from a file, with funtion specifiying how to remap storage locations in the parameter list    checkpoint = torch.load(model_file, map_location=lambda storage, loc: storage) #gpu->cpu, why?!    # 去掉字符串 ‘module.’    state_dict = {str.replace(k,'module.',''): v for k,v in checkpoint['state_dict'].items()}    # 只保留 ‘layer’ 的部分    for key in list(state_dict.keys()):        if not 'layer' in key:            del state_dict[key]            continue    model.load_state_dict(state_dict)#导入    layers = list(model.children())[-1*model.netVLADtrain:]    encoder = nn.Sequential(*layers)    model = nn.Module()    model.add_module('encoder', encoder)### 添加（初始化）model中的pooling模块    if opt.pooling.lower() == 'netvlad':        net_vlad = netvlad.NetVLAD(num_clusters=opt.num_clusters, dim=encoder_dim, vladv2=False)        if not opt.resume:            initcache = join(opt.dataPath, 'centroids', 'resnet18_' + whole_test_set.dataset + '_' + str(                opt.num_clusters) + '_desc_cen.hdf5')            if not exists(initcache):                raise FileNotFoundError('Could not find clusters, please run with --mode=cluster before proceeding')            with h5py.File(initcache, mode='r') as h5:                clsts = h5.get("centroids")[...]                traindescs = h5.get("descriptors")[...]                net_vlad.init_params(clsts, traindescs)                del clsts, traindescs        model.add_module('pool', net_vlad)    elif opt.pooling.lower() == 'max':        global_pool = nn.AdaptiveMaxPool2d((1, 1))        model.add_module('pool', nn.Sequential(*[global_pool, Flatten(), L2Norm()]))    elif opt.pooling.lower() == 'avg':        global_pool = nn.AdaptiveAvgPool2d((1, 1))        model.add_module('pool', nn.Sequential(*[global_pool, Flatten(), L2Norm()]))    else:        raise ValueError('Unknown pooling type: ' + opt.pooling)    isParallel = False    if opt.nGPU > 1 and torch.cuda.device_count() > 1:        model.encoder = nn.DataParallel(model.encoder)        model.pool = nn.DataParallel(model.pool)        isParallel = True    if not opt.resume:        model = model.to(device)## 读入netVLAD分支的预先训练结果    if opt.resume:        if opt.ckpt.lower() == 'latest':            resume_ckpt = join(opt.resume, 'checkpoints', 'checkpoint.pth.tar')        elif opt.ckpt.lower() == 'best':            resume_ckpt = join(opt.resume, 'checkpoints', 'model_best.pth.tar')        if isfile(resume_ckpt):            print("=> loading checkpoint '{}'".format(resume_ckpt))            checkpoint = torch.load(resume_ckpt, map_location=lambda storage, loc: storage)            opt.start_epoch = checkpoint['epoch']            best_metric = checkpoint['best_score']            state_dict = {k: v for k, v in checkpoint['state_dict'].items()}            # 只保留 ‘layer’ 的部分，且更新编号            for key in list(state_dict.keys()):                if ('encoder.0' in key) or ('encoder.1' in key):                    del state_dict[key]                    continue            if netVLADtrainNum == 4:                state_dict = {str.replace(k,'encoder.3','encoder.0'): v for k, v in state_dict.items()}                state_dict = {str.replace(k, 'encoder.4', 'encoder.1'): v for k, v in state_dict.items()}                state_dict = {str.replace(k, 'encoder.5', 'encoder.2'): v for k, v in state_dict.items()}                state_dict = {str.replace(k, 'encoder.6', 'encoder.3'): v for k, v in state_dict.items()}            if netVLADtrainNum <= 3:                for key in list(state_dict.keys()):                    if 'encoder.3' in key:                        del state_dict[key]                        continue                state_dict = {str.replace(k, 'encoder.4', 'encoder.0'): v for k, v in state_dict.items()}                state_dict = {str.replace(k, 'encoder.5', 'encoder.1'): v for k, v in state_dict.items()}                state_dict = {str.replace(k, 'encoder.6', 'encoder.2'): v for k, v in state_dict.items()}            if netVLADtrainNum <= 2:                for key in list(state_dict.keys()):                    if 'encoder.0' in key:                        del state_dict[key]                        continue                state_dict = {str.replace(k, 'encoder.1', 'encoder.0'): v for k, v in state_dict.items()}                state_dict = {str.replace(k, 'encoder.2', 'encoder.1'): v for k, v in state_dict.items()}            if netVLADtrainNum <= 1:                for key in list(state_dict.keys()):                    if 'encoder.0' in key:                        del state_dict[key]                        continue                state_dict = {str.replace(k, 'encoder.1', 'encoder.0'): v for k, v in state_dict.items()}            model.load_state_dict(state_dict)            model = model.to(device)            print("=> loaded checkpoint '{}' (epoch {})"                  .format(resume_ckpt, checkpoint['epoch']))        else:            print("=> no checkpoint found at '{}'".format(resume_ckpt))## 执行test/cluster/train操作    print('===> Running evaluation step')    epoch = 1    #test()    distances, predictions = testDataset(whole_test_set, epoch, write_tboard=False)    np.savetxt('distances-a1-a2.txt', distances)    np.savetxt('predictions-a1-a2.txt', predictions)